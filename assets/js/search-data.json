{
  
    
        "post0": {
            "title": "A fun short story that never happened",
            "content": "A fun short story that never happened . The Basics: . #TODO Explain the “Wing It” game and it’s rules!!! . Time used: . 35 minutes | 11.07.22 | . Situation: . You are a kindergarten teacher at a very exclusive private elementary school. While you were in the bathroom, a little girl used a magnifying glass to ignite a stack of papers, and now all three floors of the building are on fire and full of precious children. The elevators have shut down, and there’s heavy smoke in the staircase. . Resources: . Eight boxes of baking powder (soda), assorted brands | 20%-off coupon to Bed, Bath and Beyond | Parachute | Helicopter | Hang glider | Story: . The other day I learned something extraordinary about the safety features of baking soda, parachutes, and coupons. It was a regular Monday morning at the kindergarten when it struck 12:13 o’clock. That’s when I usually take a coffee break. That is, not a break to drink coffee but rather a break to get rid of the delicate side products the coffee makes me produce. Here’s something you need to know: I really like to squeeze out every ounce of my coffee break. Wait, that came out the wrong way. Man, somehow that too. I’m on a run!! Anyhow, I took my time enjoying the breaking news about how something funny like any self-respecting teacher does when my nose detected a rather discomforting smell. And no, I’m not talking about my wonderful coffee side product. It was a rather smokey smell, which is weird, because we usually don’t schedule our weekly fart lighting contest until Thursday. In short succession I heard multiple people break out in tears and cry. Again, highly unusual, since my colleague Robert usually only cries on Friday when it’s time to say goodbye to his favorite children. Moreover, he was on vacation. . As you can see, I got seriously confused. So I pulled up my pants as fast as possible and left the toilet to see what was going on. And no, I have to disappoint you, I did not forget to mention the part where I whipped my tender buttox. . Leaving the loo, I turned around and discovered that a little girl named Lilly used a magnifying glass to ignite a stack of papers, and now all three floors of the building were on fire and full of precious children. You surely understand my surprise when I saw this. Just last week I was about to send Lily’s parents a letter to notify them about her poor performance in science class and yet here she was, applying a magnifying glass to foster a world class inferno. I was proud, to say the very least! Lilly finally learned the fine arts of combustion, I’m an awesome teacher: Check! I know, I know. You’re probably thinking: “Science class in kindergarten” that’s nuts? Well, I absolutely agree, these overly ambitious helicopter parents simply find more and more ways to get their kids to become the next Elon Musk. Not that it would bother me. I’m quite glad actually. These little brats are the perfect audience for the lonesome PhD archetype that I am. I mean, common guys: I can geek around about sciency stuff all day long without having to fear any intellectual retaliation. They don’t ask any pesky questions and just nod, as if they understood. They are so much better than most of the students I’ve worked with over the years. It’s a win win. Putting aside the important stuff, we can redirect our attention to that tini tiny flame I mentioned earlier. Right, so there’s this huge fire, bunches of screaming children and a brilliant 3 year pyromaniac. So I’m thinking: my main priority is to get everybody out of the building. Well, everyone except for nosy Nate. That tiny monster recently attacked me with a specialty of his: boggers and snot soup. You can probably guess why he’s been giving the rather accurate nickname. Now I’d like to invite you to put yourself into my shoes: “Your’re on the pre to last floor and it seems like all ways down are blocked, the only rooms on this floor are the kitchen, the toilet and the super-duper shut-eye slumber sanctuary.” Yes, you heard me right, imagine saying this 3 times a day to get 30 tiny fuckers up to the 3rd floor of the building: “Hey hey you little sweethearts, it’s time to frequent the super-duper shut-eye slumber sanctuary”. I always said, we shouldn’t have appointed Kathrin to “Supreme Room Name Selector”. First of all, after she was done with her duties – on the first day of work, who could’ve imagined – it wasn’t long before she started naming other things. Then again, we have too much money anyway and Kathrin doesn’t have any talents so doing the math on this one, I think we’re fine. At the very least we’re better off because Kathrine is not let loose on the children. . Where was I? Right, that’s the wrong question. Where were you? Now that you can see in what quandary I found myself, try and ask yourself what you’d do next? Correct: “Baking powder”. That was my first instinct as well. Oh, why baking powder? I’m glad that you asked. According to Wikipedia, backing powder has an amazing ability to extinguish fires. At least that’s what I learned sitting on the toilet preparing my next science lesson. 30 kids, a tiny flame and Eight boxes of baking powder assorted brands of course, that was gonna be a hit lecture. What I didn’t know is that the flame would be substituted by a raging hell fire, but as every great teacher, I had to rise to the occasion. Besides, the hotter the flame, the starker the effect. Every good magician knows that. Although the good ones probably stay professional and call their assistants something else. Right as I was about to unpack one of the boxes of backing powder my group of kids froze in a sudden shock of realization. Some started pointing at the backing powder and started waiving their hands frantically. Taking this as a sign that they were ready for the magic, I opened up a pack and threw it in the fire. Fireworks, combustion, surprise and disgust. Those were the emotions I felt when I realized, that I’d misread the Wikipedia article. It wasn’t baking powder that extinguished fire, but rather baking soda. The powder itself is highly combustible and created an immediate shockwave of heat and disillusionment in my pupils: “Their bellowed teacher made a fatal mistake”. Taken aback by my failure and the loss of eyebrows I endured because of the jet of flame I watched in surprise as my students identified a pack of backing soda in the kitchen. They put out the flame and helped me get back on my feet to climb the staircase and make it out on the rooftop. (Wow, these tiny twats are brilliant – it’s surely not me who taught them that.) From here on it was a breeze. The kids and I were finally free, breathing some fresh air, taking in the beautiful view from our luxurious penthouse kindergarten in the Hills of Los MoneyCanBuyAnythingExceptSmartKindergartenTeachers. In a short instance we forgot that the building was crumbling underneath our feet which is why we had to act quickly. Fortunately, enough we were not alone on the rooftop. A beautiful black M-372 pantera helicopter was waiting for me to mount it. You’re probably thinking: “A helicopter in a kindergarten, yeah, as if.“ I mean yeah, we’re loaded. I did mention we’re a private kindergarten right? Damn peasants. Anyway, a couple of weeks later it’s time for the big moment. I’m getting my medal of honor. At least that’s what I was thinking. We’re standing in the majors hall. Suited up and freshly bathed I’m thinking to myself: Damn today’s the day, you’re looking hella good in your high school suit. But to be honest I possibly looked kinda funny with one quarter of an eyebrow. Anyway, my mom said that I would never need the suit again, but here I was, showing here wrong. Isn’t that what it’s all about? Instead of the long awaited medal of honor I’m given a small box with something marvelous inside. It’s a 20%-off coupon to Bed, Bath and Beyond .",
            "url": "https://kaykozaronek.github.io/blog/wing%20it/extreme%20storytelling/experiment/2022/12/07/Short-Story.html",
            "relUrl": "/wing%20it/extreme%20storytelling/experiment/2022/12/07/Short-Story.html",
            "date": " • Dec 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Weekly Goals Update 07",
            "content": "Status: . Since my last update I learned quite a few important lessons: . It&#39;s ok to be slow | Your mental health should be a top priority | In research, learnings are more important than goals | While spending time in Berkeley and San Francisco I met a plethora of rational well intentioned and intimidatingly successful people. Lots of ideas were filling the air and a shared sense of urgency was palpable. The feeling of being supposed to do impactful work, coupled with a self-perceived inability to perform at the necessary level made me sick to the stomache. At some point after staring into the abyss and questioning myself for too long I wrote the following: . I feel awful. . Everything I write is crap so I don&#39;t even send it to my mentor, even though he might be able to help. . I keep staring at my laptop and space out for what feels like hours on end. . It&#39;s the first time in 2 years that I procrastinate. This is new and inacceptable and feels like a huge step back. I watch YouTube videos and beat myself up for doing so. . Even worse, my self doubt makes me socially awkward. Instead of spending some time to talk to people, I tell myself I should work on the project and not waste even more time. Then when I sit in front of my laptop I just blank out. . I&#39;m not excited to wake up in the morning and work on anything because I dread the experience of staring at a blank laptop screen. . My mental health has deteriorated massively since I started the research fellowship. . The above marked a low point for me this year. Fortunately, my mentor was able to help me back on my feet. . One key consideration helped me get out of this rut quickly: . &quot;Whenever you feel like you cannot perform good work because of some mental barrier switch gears immediately:Go from work mode to mental health mode. Make being content your only priority.&quot; This is not to say that every time you feel like you&#39;re facing adversity you should quit and take a stroll around the block or eat some ice cream. Instead, &#39;staring at a screen for 20 minutes without doing anything productive (possibly beating yourself up for it)&#39; is a good starting point to take a break and fix your state of mind. . That being said, receiving the &quot;permission to prioritize my mental well-being&quot; changed my situation almost immediately. For everyone in similar situations I can recommend to do the same. Go have a cup of tea, take a walk, cook something delicious or just call a friend. It works wonders. . &nbsp; Goal Points Progress Total Points . 0 Full Stack Deep Learning Notebooks | 25 | 1.000000 | 25.000000 | . 1 Apply to Research Assistant Position | 15 | 1.000000 | 15.000000 | . 2 Get summary statistics on OpenAI dataset | 15 | 1.000000 | 15.000000 | . 3 Explore SF | 10 | 1.000000 | 10.000000 | . 4 Explore Palo Alto | 10 | 1.000000 | 10.000000 | . 5 Setup Weights and Biases | 5 | 1.000000 | 5.000000 | . 6 Total | | | 80.000000 | . Goals: . This week I will start consolidating what I learned over the past few weeks. . The most important thing will be to synchronize my code with Jeremys code and to prepare initial drafts for my final outputs. . Goal Points . 0 Start Consolidating Code Base | 25 | . 1 Compare experimental setups (OpenAI vs Ours) | 20 | . 2 Draft Final Post | 15 | . 3 Draft Final Deliverable Presentation | 15 | . 4 FSDL: Lab 4 | 10 | . 5 Apply to research assistant role | 10 | . 6 FSDL: Lecture 2 | 5 | . 7 Total | | . Explore in upcoming week(s): . Finish up the fellowship | .",
            "url": "https://kaykozaronek.github.io/blog/reading/books/academic%20papers/blogs/mental%20diet/2022/08/14/Update-07-Weekly-Goals.html",
            "relUrl": "/reading/books/academic%20papers/blogs/mental%20diet/2022/08/14/Update-07-Weekly-Goals.html",
            "date": " • Aug 14, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Weekly Goals Update 06",
            "content": "Status: . EAG SF went well. I met a lot of new people and friends I previously made in Prague and London. One main question I explored was whether there are success stories for how we could succeed at aligning AI. To my surprise, not a lot of people have thought about this in a satisfactory manner. Most ideas people propounded were hand wavy statements of the kind: &quot;We&#39;ll work hard on this and then hope that nothing breaks. And because it&#39;s a hard problem we&#39;ll work really hard and this will make us succeed.&quot; . Additionally, I explored the question of whether I can become a world-class ML engineer and unfortunately there was no conclusive evidence for or against it. There is a lack of people I can compare myself against, which makes it harder to assess my skill level proportional to the time I invested in building it. In the end, the main take away is probably, that it&#39;s too early to tell. . &nbsp; Goal Points Progress Total Points . 0 Have 10 great conversations at EAG SF with new friends | 25 | 1.000000 | 25.000000 | . 1 Read AGI Safety from first principles | 10 | 0.500000 | 5.000000 | . 2 Get summary statistics on OpenAI dataset | 15 | 0.000000 | 0.000000 | . 3 Plan stay in San Francisco | 5 | 1.000000 | 5.000000 | . 4 Finish Reading Ajeya Cotra&#39;s Blogpost | 15 | 1.000000 | 15.000000 | . 5 Meet FAR team at EAG | 5 | 1.000000 | 5.000000 | . 6 Setup Weights and Biases | 5 | 0.000000 | 0.000000 | . 7 Use OpenAI playground to explore different prompting techniques (e.g. Zero-Shot CoT) | 5 | 1.000000 | 5.000000 | . 8 Skim &quot;Language models are zero-shot reasoners&quot; paper | 5 | 1.000000 | 5.000000 | . 9 Skim &quot;Chain of thought reasoning&quot; paper | 5 | 1.000000 | 5.000000 | . 10 Skim Minerva paper | 5 | 1.000000 | 5.000000 | . 11 Total | | | 75.000000 | . Goals: . This week I will focus on catching up with new acquaintances from EAG. . Apart from that I want to get set up for the Full Stack Deep Learning Course that starts next week. . Goal Points . 0 Full Stack Deep Learning Notebooks | 25 | . 1 Apply to Research Assistant Position | 15 | . 2 Get summary statistics on OpenAI dataset | 15 | . 3 Explore SF | 10 | . 4 Explore Palo Alto | 10 | . 5 Setup Weights and Biases | 5 | . 6 Total | | . Explore in upcoming week(s): . Explore San Francisco and Berkeley | .",
            "url": "https://kaykozaronek.github.io/blog/reading/books/academic%20papers/blogs/mental%20diet/2022/07/31/Update-06-Weekly-Goals.html",
            "relUrl": "/reading/books/academic%20papers/blogs/mental%20diet/2022/07/31/Update-06-Weekly-Goals.html",
            "date": " • Jul 31, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Weekly Goals Update 05",
            "content": "Status: . Last week I struggled a lot with imposter syndrome. I felt like I didn&#39;t progress at a proper speed. A key realization helped me get over this and accept that research is inherently non-linear. After all: &quot;The real world is not an online course&quot;. . The main difficulty in doing self directed, open-ended work is that you&#39;re iteratively refining your task i.e. the problem statement and your solution attempts. After finding an initial problem to tackle, you quickly find out that either you don&#39;t have the right tools to tackle the problem or you don&#39;t understand how to apply your tools to solve the problem. In any case you go back and try to define the problem a little more succinctly such that you arrive at a more &quot;atomic&quot; subproblem, which you can try to solve. . In light of this realization, it pays to take some time and try to identify the most important problems that pose bottlenecks. Breaking them down into smaller bits takes some time, but makes you progress faster in the long run. . Moreover, breaking down your problems helps you make sense of what confuses you. This is crucial for being able to effectively make use of your mentor. Only if you&#39;re well prepared you&#39;ll be able to ask the right questions to help you move forward. . &nbsp; Goal Points Progress Total Points . 0 Create Project Github Repository | 5 | 1.000000 | 5.000000 | . 1 Come up with project folder structure | 10 | 1.000000 | 10.000000 | . 2 Use Weights and Biases in project | 10 | 0.300000 | 3.000000 | . 3 Analyze OpenAI dataset | 15 | 1.000000 | 15.000000 | . 4 Publish last weeks paper reimplementation | 10 | 1.000000 | 10.000000 | . 5 Extend code to entire dataset | 20 | 0.500000 | 10.000000 | . 6 Publish REINFORCE exercise | 15 | 0.000000 | 0.000000 | . 7 Read &quot;Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover&quot; | 15 | 0.600000 | 9.000000 | . 8 Total | | | 62.000000 | . Goals: . This week I will focus on making new friends and getting settled in San Francisco. While traveling, I won&#39;t be able to code a lot which is why I will spend a lot of time reading. . I want to have at least 10 in-depth conversations (1 on 1&#39;s at EAG). At the end of this week I should have a clearer view of where to work (coworking space), whom to meet after the conference and what to explore in the Bay Area and beyond (e.g. Yosemite national park) . Goal Points . 0 Have 10 great conversations at EAG SF with new... | 25 | . 1 Read AGI Safety from first principles | 10 | . 2 Get summary statistics on OpenAI dataset | 15 | . 3 Plan stay in San Francisco | 5 | . 4 Finish Reading Ajeya Cotra&#39;s Blogpost | 15 | . 5 Meet FAR team at EAG | 5 | . 6 Setup Weights and Biases | 5 | . 7 Use OpenAI playground to explore different pro... | 5 | . 8 Skim &quot;Language models are zero-shot reasoners&quot;... | 5 | . 9 Skim &quot;Chain of thought reasoning&quot; paper | 5 | . 10 Skim Minerva paper | 5 | . 11 Total | | . Explore in upcoming week(s): . Explore San Francisco and Berkeley | .",
            "url": "https://kaykozaronek.github.io/blog/reading/books/academic%20papers/blogs/mental%20diet/2022/07/24/Update-05-Weekly-Goals.html",
            "relUrl": "/reading/books/academic%20papers/blogs/mental%20diet/2022/07/24/Update-05-Weekly-Goals.html",
            "date": " • Jul 24, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Training GPT 3 with language feedback",
            "content": "Imports . import pandas as pd import openai from datasets import load_dataset import os import string import numpy as np from sklearn.metrics.pairwise import cosine_similarity from typing import List # from tenacity import retry, wait_random_exponential, stop_after_attempt . WARNING: Remove the API key after running the cell and clear output so it does not get logged to wandb in case you sync code (see settings) . %env OPENAI_API_KEY= . openai.api_key = os.getenv(&quot;OPENAI_API_KEY&quot;) . Understand the original datasets . Let&#39;s have a look at the dataset the authors used to see what it is that we want to replicate. . Initial summaries . initial_summaries = load_dataset(&#39;json&#39;, data_files=[&#39;data/language_feedback_learning_dataset/finetuning/original_summary_train.jsonl&#39;])[&#39;train&#39;] . We can see that the prompt contains: . Instruction | Title: | Text: | TL;DR: | . The prompt was created like so: . . print(initial_summaries[&#39;prompt&#39;][0]) . Write an excellent summary of a given text. An excellent summary is coherent, accurate, concise, and detailed, and it follows human preferences about summaries. Title: Friend [25M] is married, but can&#39;t get over feelings for me [22F]. Text: I (22f) was briefly enrolled in a graduate program this fall, and am still in close contact with a lot of friends I made while I was there. One of these friends is a 25 year old guy from China. Because English was not his first language and I could tell he was a little isolated, I was careful to be nice and invite him to events. We started hanging out some on our own, but mostly just to discuss school/world politics; nothing too personal. He asked me out a few times this fall, and even though I assumed he was single, I turned him down nicely-- just wasn&#39;t interested. After the final time I turned him down he got a little clingy and weird, so I tried to create some space between us; I didn&#39;t really speak to him over Christmas break. Imagine my surprise when my presumably single friend comes home from China married after Christmas! Turns out he&#39;d had a girlfriend all along and they tied the knot while he was there. I assumed that settled things, and went back to occasionally spending time with him. Last night, he admitted to me that he&#39;s jealous of the guy I&#39;ve been dating and has feelings for me. I sort of just apologized and ignored it, but now I&#39;m frustrated and not sure what to do. Normally, I would just cut things off with him since he doesn&#39;t seem to be able to have &#34;just friends&#34; feelings toward me. In this case, though, I feel guilty, since he really doesn&#39;t have many friends and seems to put a lot of energy into maintaining contact with me. So what should I do? Just end things? Pretend it never happened? All I know is that I certainly wouldn&#39;t want MY husband spending time alone with a woman he had feelings for. I don&#39;t want to cross any lines. TL;DR: . The completion is the initial summary the model produces . print(initial_summaries[&#39;completion&#39;][0]) . My friend from school who is now married is still in love with me and I don&#39;t know what to do about it. . Summaries and Feedback . All of the above data can be gathered and a human can provide feedback on the initial summary. . You can see the result below. . This is the intermediary step that gets us from the original summary to the summaries created after feedback (refinements). . df = pd.read_csv(&#39;data/language_feedback_learning_dataset/refinements/summaries_and_feedback.csv&#39;) df.head(2) . id post title subreddit prompt summary feedback . 0 t3_18tffu | I (22f) was briefly enrolled in a graduate pr... | Friend [25M] is married, but can&#39;t get over fe... | relationships | Write an excellent summary of a given text. An... | My friend from school who is now married is st... | This summary is good overall, it should howeve... | . 1 t3_3aegv4 | We&#39;ve been dating for 4 months, but we knew ea... | Me [34/F] got frustrated with my boyfriend [34... | relationship_advice | Write an excellent summary of a given text. An... | A woman discusses her frustration with her boy... | This summary is good, it should however point ... | . Summaries after feedback (refinements) . In this file we can see the refinements. That is, the summaries produced after the model has been given feedback on it&#39;s initial summary. . The refinements were obtained with the following prompt: . . Important Note: . The prompt in the feedback_refinement_train.jsonl file does not look like the prompt used to obtain the refinement with feedback. . The prompt rather resembles the structure found in the initial summariesprompt. . Concretely, this means, that the Summary: and Feedback: are not included in the prompt. . This might seem odd at first, but it makes sense if you think about what the data will look like at test time. . During training we have access to both the summary and the feedback. This is not the case during testing where you only have access to the instruction, title and text. . feedback_refinement = load_dataset(&#39;json&#39;, data_files=[&#39;data/language_feedback_learning_dataset/finetuning/feedback_refinement_train.jsonl&#39;])[&#39;train&#39;] . print(feedback_refinement[&#39;prompt&#39;][0]) . Write an excellent summary of a given text. An excellent summary is coherent, accurate, concise, and detailed, and it follows human preferences about summaries. Title: Friend [25M] is married, but can&#39;t get over feelings for me [22F]. Text: I (22f) was briefly enrolled in a graduate program this fall, and am still in close contact with a lot of friends I made while I was there. One of these friends is a 25 year old guy from China. Because English was not his first language and I could tell he was a little isolated, I was careful to be nice and invite him to events. We started hanging out some on our own, but mostly just to discuss school/world politics; nothing too personal. He asked me out a few times this fall, and even though I assumed he was single, I turned him down nicely-- just wasn&#39;t interested. After the final time I turned him down he got a little clingy and weird, so I tried to create some space between us; I didn&#39;t really speak to him over Christmas break. Imagine my surprise when my presumably single friend comes home from China married after Christmas! Turns out he&#39;d had a girlfriend all along and they tied the knot while he was there. I assumed that settled things, and went back to occasionally spending time with him. Last night, he admitted to me that he&#39;s jealous of the guy I&#39;ve been dating and has feelings for me. I sort of just apologized and ignored it, but now I&#39;m frustrated and not sure what to do. Normally, I would just cut things off with him since he doesn&#39;t seem to be able to have &#34;just friends&#34; feelings toward me. In this case, though, I feel guilty, since he really doesn&#39;t have many friends and seems to put a lot of energy into maintaining contact with me. So what should I do? Just end things? Pretend it never happened? All I know is that I certainly wouldn&#39;t want MY husband spending time alone with a woman he had feelings for. I don&#39;t want to cross any lines. TL;DR: . print(feedback_refinement[&#39;completion&#39;][0]) . My friend from school who is now married is still in love with me and I don&#39;t know what to do about it. He&#39;s jealous of the guy I&#39;ve been dating and I feel guilty for trying to cut things off. . Replicate the algorithm . Ok, now that we know how the data looks like we can start replicating the results. . I will walk through the algorithm with a single example such that the main idea will be conveyed while keeping the code easy to understand. . . 0: A language model generates an output. A human writes feedback on the output. . Let&#39;s get our model to generate the initial summaries. . To do so we have to create an appropriate prompt. It will look like so: . . Let&#39;s reuse the data from the summaries_and_feedback.csv file. . We can take one example and use it to exemplify the pipeline. . df.head(1) . id post title subreddit prompt summary feedback . 0 t3_18tffu | I (22f) was briefly enrolled in a graduate pr... | Friend [25M] is married, but can&#39;t get over fe... | relationships | Write an excellent summary of a given text. An... | My friend from school who is now married is st... | This summary is good overall, it should howeve... | . This is what we&#39;re aiming for: . print(df[&#39;prompt&#39;][0]) . Write an excellent summary of a given text. An excellent summary is coherent, accurate, concise, and detailed, and it follows human preferences about summaries. Title: Friend [25M] is married, but can&#39;t get over feelings for me [22F]. Text: I (22f) was briefly enrolled in a graduate program this fall, and am still in close contact with a lot of friends I made while I was there. One of these friends is a 25 year old guy from China. Because English was not his first language and I could tell he was a little isolated, I was careful to be nice and invite him to events. We started hanging out some on our own, but mostly just to discuss school/world politics; nothing too personal. He asked me out a few times this fall, and even though I assumed he was single, I turned him down nicely-- just wasn&#39;t interested. After the final time I turned him down he got a little clingy and weird, so I tried to create some space between us; I didn&#39;t really speak to him over Christmas break. Imagine my surprise when my presumably single friend comes home from China married after Christmas! Turns out he&#39;d had a girlfriend all along and they tied the knot while he was there. I assumed that settled things, and went back to occasionally spending time with him. Last night, he admitted to me that he&#39;s jealous of the guy I&#39;ve been dating and has feelings for me. I sort of just apologized and ignored it, but now I&#39;m frustrated and not sure what to do. Normally, I would just cut things off with him since he doesn&#39;t seem to be able to have &#34;just friends&#34; feelings toward me. In this case, though, I feel guilty, since he really doesn&#39;t have many friends and seems to put a lot of energy into maintaining contact with me. So what should I do? Just end things? Pretend it never happened? All I know is that I certainly wouldn&#39;t want MY husband spending time alone with a woman he had feelings for. I don&#39;t want to cross any lines. TL;DR . def generate_prompt(instructions, title, text): prompt = instructions + &quot; n nTitle: &quot; + title + &quot; n nText: &quot; + text + &#39; n nTL;DR&#39; return prompt . instructions = &quot;Write an excellent summary of a given text. An excellent summary is coherent, accurate, concise, and detailed, and it follows human preferences about summaries.&quot; title = df[&#39;title&#39;][0] text = df[&#39;post&#39;][0] prompt = generate_prompt(instructions, title, text) print(prompt) . Write an excellent summary of a given text. An excellent summary is coherent, accurate, concise, and detailed, and it follows human preferences about summaries. Title: Friend [25M] is married, but can&#39;t get over feelings for me [22F]. Text: I (22f) was briefly enrolled in a graduate program this fall, and am still in close contact with a lot of friends I made while I was there. One of these friends is a 25 year old guy from China. Because English was not his first language and I could tell he was a little isolated, I was careful to be nice and invite him to events. We started hanging out some on our own, but mostly just to discuss school/world politics; nothing too personal. He asked me out a few times this fall, and even though I assumed he was single, I turned him down nicely-- just wasn&#39;t interested. After the final time I turned him down he got a little clingy and weird, so I tried to create some space between us; I didn&#39;t really speak to him over Christmas break. Imagine my surprise when my presumably single friend comes home from China married after Christmas! Turns out he&#39;d had a girlfriend all along and they tied the knot while he was there. I assumed that settled things, and went back to occasionally spending time with him. Last night, he admitted to me that he&#39;s jealous of the guy I&#39;ve been dating and has feelings for me. I sort of just apologized and ignored it, but now I&#39;m frustrated and not sure what to do. Normally, I would just cut things off with him since he doesn&#39;t seem to be able to have &#34;just friends&#34; feelings toward me. In this case, though, I feel guilty, since he really doesn&#39;t have many friends and seems to put a lot of energy into maintaining contact with me. So what should I do? Just end things? Pretend it never happened? All I know is that I certainly wouldn&#39;t want MY husband spending time alone with a woman he had feelings for. I don&#39;t want to cross any lines. TL;DR . Let&#39;s have a look at whether the prompts are identical . prompt == df[&#39;prompt&#39;][0] . True . Great, now that we know how the prompt is generated, we can generate an output: our initial summary. . From the paper we get some useful information: . We take 100 samples from the Reddit data subset used in Stiennon et al. (2020). We use InstructGPT (175B) to generate initial summaries and refinements, using the instructions in Appendix F. We then write feedback f on the initial summary y given the Reddit post x, and we generate possible refinements $y′_{1}$, . . . , $y′_{20}$. . In Appendix E.1 we find more on how to generate the desired outputs: . For all summarization experiments (§3.2), we sample up to 48 tokens (as in Stiennon et al., 2020) with nucleus sampling (Holtzman et al., 2019) with p = 0.9. We strip nonalphanumeric characters (e.g., newlines) from the beginning of sampled summaries. Due to the maximum token length, sampled summaries sometimes end with an incomplete sentence. Thus, we remove ending sentences that do not end in “.”, “!”, or “?” . To summarize: . model: &#39;InstructGPT (175B)&#39; -&gt; text-davinci-001 | max_tokens: 48 | sampling: &#39;nucleus sampling with p=0.9&#39; -&gt; top_p=0.9 | number of generations for refinements: 20 -&gt; n=20 | number of generations for initial summary: 1 -&gt; n=1 | remove newline characters from beginning of summary | remove summaries if they are incomplete sentences | . The resulting call to the OpenAI API looks like this . model_output_choices = openai.Completion.create( model=&quot;text-davinci-001&quot;, prompt=input_text, max_tokens=48, top_p=0.9, n=1) # n=20 for refinement generation . initial_summary_output = openai.Completion.create( model=&quot;text-davinci-001&quot;, prompt=prompt, max_tokens=48, top_p=0.9, n=1, stop=&#39; n&#39;) . initial_summary = initial_summary_output[&#39;choices&#39;][0][&#39;text&#39;] initial_summary . &#39; Friend from graduate program married, now has feelings for friend; what should she do?&#39; . Let&#39;s perform the remaining 2 processing steps . remove newline characters from beginning of summary | remove summaries if they are incomplete sentences | . string.whitespace + string.punctuation . &#39; t n r x0b x0c!&#34;#$%&amp; &#39;()*+,-./:;&lt;=&gt;?@[ ]^_`{|}~&#39; . proccessed_summary = initial_summary.lstrip(string.whitespace + string.punctuation) proccessed_summary . &#39;Friend from graduate program married, now has feelings for friend; what should she do?&#39; . Since our summary ends with one of the valid signs we don&#39;t need to prduce another summary. . valid_end_signs = (&#39;.&#39;, &#39;!&#39;, &#39;?&#39;) proccessed_summary.endswith(valid_end_signs) . True . As a last task in step 0 a human writes feedback on the output. . We could write this ourselves, but I think the main gist is clear. | The main point is that this step is performed manually by humans and can therefore be quite costly. | We&#39;ll therefore just use the feedback from the dataset | . feedback = df[&#39;feedback&#39;][0] feedback . &#39;This summary is good overall, it should however mention that her friend from graduate school does not have many friends and that she feels guilty for trying to cut things off. Also her friend puts a lot of energy in maintaining contact with the author. The summary should also convey that the author is asking for advice on what to do about this. &#39; . 1 Condition the language model on the input, output, and feedback to generate multiple refinements. . Now that we have created an initial summary and tasked humans to provide written feedback on them, we can prompt the model again to create refinements . We can reuse our prompt creation function and modify it a little. . def generate_prompt(instructions, title, text, is_refinement=False, summary=None, feedback=None): if is_refinement: prompt = instructions + &quot; n nTitle: &quot; + title + &quot; n nText: &quot; + text + &#39; n nSummary:&#39; + summary + &#39; n nFeedback:&#39; + feedback + &#39; n nTL;DR&#39; else: prompt = instructions + &quot; n nTitle: &quot; + title + &quot; n nText: &quot; + text + &#39; n nTL;DR&#39; return prompt . refinement_instructions = &quot;&quot;&quot;Given a text, an initial summary, and feedback on the initial summary, write an improved summary that incorporates the feedback on the initial summary and is better than the initial summary. The improved summary is coherent, accurate, concise, and detailed, and it follows human preferences about summaries. Most importantly, the improved summary incorporates the feedback. &quot;&quot;&quot; refinement_prompt = generate_prompt(refinement_instructions, title, text, is_refinement=True, summary=proccessed_summary, feedback=feedback ) . We can use the new prompt to get 20 refinements. . model_output_choices = openai.Completion.create( model=&quot;text-davinci-001&quot;, prompt=refinement_prompt, max_tokens=48, top_p=0.9, n=20) . Congrats, you&#39;ve just generated 20 refinements . refinements = pd.DataFrame(model_output_choices[&#39;choices&#39;])[[&#39;text&#39;]] refinements . text . 0 : n nThe author has a friend from their gradua... | . 1 : n nThe author&#39;s friend from graduate school ... | . 2 : n nThe author&#39;s friend from graduate school... | . 3 : The author has a friend from graduate school... | . 4 : n nThe author is in close contact with a lo... | . 5 : n nThe author has a friend from graduate sch... | . 6 : n nThe author has a friend from graduate sc... | . 7 : n nThe author has a friend from graduate sc... | . 8 : n nThe author has a friend from their gradu... | . 9 : n nThe author has a friend from graduate sc... | . 10 : n nThe author is in close contact with a fr... | . 11 : n nThe author has a friend from graduate sch... | . 12 : n nThe author&#39;s friend from graduate school... | . 13 : n nThe author has a friend from graduate sch... | . 14 : n nThe author has a friend from graduate sc... | . 15 : n nThe author has a friend from graduate sc... | . 16 : n nThe author&#39;s friend from graduate school ... | . 17 : n nThe author&#39;s friend from graduate school... | . 18 : n nThe author&#39;s friend from graduate school ... | . 19 : n nThe author has a friend from graduate sc... | . Let&#39;s perform the remaining 2 processing steps . remove newline characters from beginning of summary | remove summaries if they are incomplete sentences | . def postprocess_refinements(refinements, valid_end_signs=(&#39;!&#39;, &#39;.&#39;, &#39;?&#39;)): # This function works on dataframes refinements[&#39;text&#39;] = refinements[&#39;text&#39;].apply(lambda x: x.lstrip(string.whitespace + string.punctuation)) refinements[&#39;text&#39;] = refinements[&#39;text&#39;].apply(lambda x: x if x.endswith(valid_end_signs) else &#39;&#39;) refinements[&#39;text&#39;].replace(&#39;&#39;, np.nan, inplace=True) refinements[&#39;text&#39;].dropna(inplace=True) return refinements . refinements_df = postprocess_refinements(refinements) refinements_df . text . 0 The author has a friend from their graduate pr... | . 1 The author&#39;s friend from graduate school is ma... | . 3 The author has a friend from graduate school w... | . 5 The author has a friend from graduate school w... | . 7 The author has a friend from graduate school w... | . 8 The author has a friend from their graduate pr... | . 9 The author has a friend from graduate school w... | . 11 The author has a friend from graduate school w... | . 13 The author has a friend from graduate school w... | . 14 The author has a friend from graduate school w... | . 15 The author has a friend from graduate school w... | . 16 The author&#39;s friend from graduate school is ma... | . 17 The author&#39;s friend from graduate school is ma... | . 18 The author&#39;s friend from graduate school is ma... | . 19 The author has a friend from graduate school w... | . 2 Choose the refinement with the highest similarity with the feedback . With the refinements in hand we can check which one has the highest similarity with our feedback and then save it as the one we&#39;ll finetune our model on. . We start by creating the get_embedding function. . It uses the openai.Embedding endpoint which is a contrastive pre-trained text embedding function. . def get_embedding(text: str, engine=&quot;text-similarity-davinci-001&quot;) -&gt; List[float]: # replace newlines, which can negatively affect performance. text = text.replace(&quot; n&quot;, &quot; &quot;) return openai.Embedding.create(input=[text], engine=engine)[&quot;data&quot;][0][&quot;embedding&quot;] print(len(get_embedding(&quot;This is a sample query text&quot;, engine=&quot;text-similarity-babbage-001&quot;))) . 2048 . Get refinement embeddings and save them for future reuse . refinements_df[&#39;babbage_similarity&#39;] = refinements_df[&#39;text&#39;].apply(lambda x: get_embedding(x, engine=&#39;text-similarity-babbage-001&#39;)) # refinements_df[&#39;babbage_search&#39;] = refinements_df[&#39;text&#39;].apply(lambda x: get_embedding(x, engine=&#39;text-search-babbage-doc-001&#39;)) refinements_df.to_csv(&#39;refinements.csv&#39;) . refinements_df.head(2) . text babbage_similarity . 0 The author has a friend from their graduate pr... | [0.004192444495856762, 0.015246931463479996, -... | . 1 The author&#39;s friend from graduate school is ma... | [0.006996044889092445, 0.01488018874078989, -0... | . len(refinements_df[&#39;babbage_similarity&#39;][0]) . 2048 . We use the sklearn cosine similarity which expects a list of lists like so. . cosine_similarity([*refinements_df[&#39;babbage_similarity&#39;]], [*refinements_df[&#39;babbage_similarity&#39;]])[:5] . array([[1. , 0.98391286, 0.93524358, 0.93785964, 0.93905301, 1. , 0.93785964, 0.93785964, 0.93524358, 0.92771537, 0.93550934, 0.98391286, 0.98391286, 0.98391286, 0.93785964], [0.98391286, 1. , 0.92949444, 0.93065931, 0.94512564, 0.98391286, 0.93065931, 0.93065931, 0.92949444, 0.93515841, 0.92720113, 1. , 1. , 1. , 0.93065931], [0.93524358, 0.92949444, 1. , 0.99801855, 0.95988518, 0.93524358, 0.99801855, 0.99801855, 1. , 0.9389564 , 0.99811568, 0.92949444, 0.92949444, 0.92949444, 0.99801855], [0.93785964, 0.93065931, 0.99801855, 1. , 0.95893281, 0.93785964, 1. , 1. , 0.99801855, 0.93808102, 0.99652958, 0.93065931, 0.93065931, 0.93065931, 1. ], [0.93905301, 0.94512564, 0.95988518, 0.95893281, 1. , 0.93905301, 0.95893281, 0.95893281, 0.95988518, 0.98025795, 0.95766883, 0.94512564, 0.94512564, 0.94512564, 0.95893281]]) . Create function to compare refinements and human feedback . def compare_refinement_to_human_feedback(refinements_df, human_feedback, n=1): embedding = get_embedding(human_feedback, engine=&#39;text-similarity-babbage-001&#39;) refinements_df[&#39;similarities&#39;] = refinements_df[&#39;babbage_similarity&#39;].apply(lambda x: cosine_similarity([x], [embedding])) res = refinements_df.sort_values(&#39;similarities&#39;, ascending=False).head(n) return res res = compare_refinement_to_human_feedback(refinements_df, feedback, n=3) . res . text babbage_similarity similarities . 5 The author has a friend from graduate school w... | [0.0005159178399480879, 0.00046537088928744197... | [[0.8158968234048924]] | . 9 The author has a friend from graduate school w... | [0.0005159178399480879, 0.00046537088928744197... | [[0.8158968234048924]] | . 11 The author has a friend from graduate school w... | [0.0005159178399480879, 0.00046537088928744197... | [[0.8158968234048924]] | . Pick refinement with highest similarity . improved_output = res.iloc[0][&#39;text&#39;] print(improved_output) . The author has a friend from graduate school who is married but has feelings for her. The author is asking for advice on what to do about this situation. . Now we have essentially replicated all the necessary steps to arrive at the feedback_refinement dataset. . We&#39;ll only need to get our data in the correct jsonl format with a prompt and completion. . 3 Finetune a language model on the improved outputs . To perform the finetuning we can consult the OpenAI API, which states that: . Your dataset must be formatted as a JSONL file, where each training example is a JSON object with the keys &quot;prompt&quot; and &quot;completion&quot;. Additionally, you must upload your file with the purpose fine-tune. . Since we now know how the dataset has been created we can use the feedback_refinement.jsonl file for the finetuning. . Let&#39;s have a look how the dataset from the paper looks like . feedback_refinement . Dataset({ features: [&#39;prompt&#39;, &#39;completion&#39;], num_rows: 100 }) . print(feedback_refinement[0][&#39;prompt&#39;]) . Write an excellent summary of a given text. An excellent summary is coherent, accurate, concise, and detailed, and it follows human preferences about summaries. Title: Friend [25M] is married, but can&#39;t get over feelings for me [22F]. Text: I (22f) was briefly enrolled in a graduate program this fall, and am still in close contact with a lot of friends I made while I was there. One of these friends is a 25 year old guy from China. Because English was not his first language and I could tell he was a little isolated, I was careful to be nice and invite him to events. We started hanging out some on our own, but mostly just to discuss school/world politics; nothing too personal. He asked me out a few times this fall, and even though I assumed he was single, I turned him down nicely-- just wasn&#39;t interested. After the final time I turned him down he got a little clingy and weird, so I tried to create some space between us; I didn&#39;t really speak to him over Christmas break. Imagine my surprise when my presumably single friend comes home from China married after Christmas! Turns out he&#39;d had a girlfriend all along and they tied the knot while he was there. I assumed that settled things, and went back to occasionally spending time with him. Last night, he admitted to me that he&#39;s jealous of the guy I&#39;ve been dating and has feelings for me. I sort of just apologized and ignored it, but now I&#39;m frustrated and not sure what to do. Normally, I would just cut things off with him since he doesn&#39;t seem to be able to have &#34;just friends&#34; feelings toward me. In this case, though, I feel guilty, since he really doesn&#39;t have many friends and seems to put a lot of energy into maintaining contact with me. So what should I do? Just end things? Pretend it never happened? All I know is that I certainly wouldn&#39;t want MY husband spending time alone with a woman he had feelings for. I don&#39;t want to cross any lines. TL;DR: . feedback_refinement[0][&#39;completion&#39;] . &#34; My friend from school who is now married is still in love with me and I don&#39;t know what to do about it. He&#39;s jealous of the guy I&#39;ve been dating and I feel guilty for trying to cut things off.&#34; . Data preparation tool . To prepare your dataset you can use the finetunes.prepare_data tool provided by OpenAI. The procedure was taken from this notebook. . !openai tools fine_tunes.prepare_data -f data/language_feedback_learning_dataset/finetuning/feedback_refinement_train.jsonl -q . Analyzing... - Your file contains 100 prompt-completion pairs - All prompts end with suffix ` n nTL;DR:` - All prompts start with prefix `Write an excellent summary of a given text. An excellent summary is coherent, accurate, concise, and detailed, and it follows human preferences about summaries. Title: `. Fine-tuning doesn&#39;t require the instruction specifying the task, or a few-shot example scenario. Most of the time you should only add the input data into the prompt, and the desired output into the completion - Your data does not contain a common ending at the end of your completions. Having a common ending string appended to the end of the completion makes it clearer to the fine-tuned model where the completion should end. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. Based on the analysis we will perform the following actions: - [Recommended] Remove prefix `Write an excellent summary of a given text. An excellent summary is coherent, accurate, concise, and detailed, and it follows human preferences about summaries. Title: ` from all prompts [Y/n]: Y - [Recommended] Add a suffix ending ` n` to all completions [Y/n]: Y Your data will be written to a new JSONL file. Proceed [Y/n]: Y Wrote modified file to `data/language_feedback_learning_dataset/finetuning/feedback_refinement_train_prepared.jsonl` Feel free to take a look! Now use that file when fine-tuning: &gt; openai api fine_tunes.create -t &#34;data/language_feedback_learning_dataset/finetuning/feedback_refinement_train_prepared.jsonl&#34; After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string ` n nTL;DR:` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[&#34; n&#34;]` so that the generated texts ends at the expected place. Once your model starts training, it&#39;ll approximately take 3.82 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you. . After having prepared the dataset, we can perform the fine-tuning. . To replicate the exact results from the paper we would use the model davinci. . That can get quite expensive though, so I opted for the cheaper ada model which brings the point home just as much. . Copy the path to the modified file and voila. . !openai api fine_tunes.create -t data/language_feedback_learning_dataset/finetuning/feedback_refinement_train_prepared.jsonl -m ada . Uploaded file from data/language_feedback_learning_dataset/finetuning/feedback_refinement_train_prepared.jsonl: file-EhRY9iXC2WRION3J6TpF8BXx Created fine-tune: ft-kkNf0oWN77w7uhOz9sefREIG Streaming events until fine-tuning is complete... (Ctrl-C will interrupt the stream, but not cancel the fine-tune) [2022-07-19 21:25:28] Created fine-tune: ft-kkNf0oWN77w7uhOz9sefREIG [2022-07-19 21:25:40] Fine-tune costs $0.06 [2022-07-19 21:25:40] Fine-tune enqueued. Queue number: 0 [2022-07-19 21:25:45] Fine-tune started [2022-07-19 21:26:13] Completed epoch 1/4 [2022-07-19 21:26:28] Completed epoch 2/4 [2022-07-19 21:26:42] Completed epoch 3/4 [2022-07-19 21:26:57] Completed epoch 4/4 [2022-07-19 21:27:17] Uploaded model: ada:ft-academicsnyuperez-2022-07-19-19-27-17 [2022-07-19 21:27:18] Uploaded result file: file-XlXM6HnYgcuxHbSgFnqtX5f2 [2022-07-19 21:27:18] Fine-tune succeeded Job complete! Status: succeeded 🎉 Try out your fine-tuned model: openai api completions.create -m ada:ft-academicsnyuperez-2022-07-19-19-27-17 -p &lt;YOUR_PROMPT&gt; . Upload progress: 0%| | 0.00/157k [00:00&lt;?, ?it/s] Upload progress: 100%|██████████| 157k/157k [00:00&lt;00:00, 154Mit/s] . The fine-tune cost us $0.06. . Let&#39;s see how we can use the model for inference. . For a quick sanity check we can reuse the prompt from above. . openai.Completion.create( model=&quot;ada:ft-academicsnyuperez-2022-07-19-19-27-17&quot;, prompt=prompt, max_tokens=48, top_p=0.9, n=1) . &lt;OpenAIObject text_completion id=cmpl-5VnCm7IbpGKNsLGp3R5t0IU18m1GX at 0x1c7e12fd3a0&gt; JSON: { &#34;choices&#34;: [ { &#34;finish_reason&#34;: &#34;length&#34;, &#34;index&#34;: 0, &#34;logprobs&#34;: null, &#34;text&#34;: &#34;: The author is trying to get a friend back from a relationship because he is afraid that if he does not break things, his friend may find out. The friend is jealously over the author&#39;s new friend and the author is unsure&#34; } ], &#34;created&#34;: 1658258996, &#34;id&#34;: &#34;cmpl-5VnCm7IbpGKNsLGp3R5t0IU18m1GX&#34;, &#34;model&#34;: &#34;ada:ft-academicsnyuperez-2022-07-19-19-27-17&#34;, &#34;object&#34;: &#34;text_completion&#34;, &#34;usage&#34;: { &#34;completion_tokens&#34;: 48, &#34;prompt_tokens&#34;: 459, &#34;total_tokens&#34;: 507 } } .",
            "url": "https://kaykozaronek.github.io/blog/gpt%203/human%20feedback/nlp/2022/07/19/Training-GPT-3.html",
            "relUrl": "/gpt%203/human%20feedback/nlp/2022/07/19/Training-GPT-3.html",
            "date": " • Jul 19, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Weekly Goals Update 04",
            "content": "Status: . &nbsp; Goal Points Progress Total Points . 0 Work through OpenAI API guides | 30 | 1.000000 | 30.000000 | . 1 Reimplement &quot;training language models with language feedback&quot; | 25 | 0.700000 | 17.500000 | . 2 Publish Transformer Exercise | 15 | 1.000000 | 15.000000 | . 3 NYU Deep Learning Course Week 12 | 20 | 1.000000 | 20.000000 | . 4 Attend EA meeting in Budapest | 5 | 1.000000 | 5.000000 | . 5 Create standard weekly schedule | 5 | 1.000000 | 5.000000 | . 6 Bonus: Check out Deep RL course | 5 | 1.000000 | 5.000000 | . 7 Total | | | 97.500000 | . Goals: . This week my main goal is to finish the reimplementation of the paper &quot;Training Language Models with Language Feedback&quot; paper. After doing so, I will refactor the code and create a Github repository for the project. . Goal Points . 0 Create Project Github Repository | 5 | . 1 Analyze OpenAI dataset | 15 | . 2 Publish last weeks paper reimplementation | 10 | . 3 Extend code to entire dataset | 20 | . 4 Publish REINFORCE exercise | 15 | . 5 Total | | . Explore in upcoming week(s): . Mastering Pytorch | .",
            "url": "https://kaykozaronek.github.io/blog/reading/books/academic%20papers/blogs/mental%20diet/2022/07/17/Update-04-Weekly-Goals.html",
            "relUrl": "/reading/books/academic%20papers/blogs/mental%20diet/2022/07/17/Update-04-Weekly-Goals.html",
            "date": " • Jul 17, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Transformer from Scratch",
            "content": "&quot;This exercise guides you to a full implementation of a transformer with Pytorch and HuggingFace&quot; . Imports . from transformers import AutoTokenizer from transformers import AutoModel from transformers import AutoConfig from torch import nn import torch.nn.functional as F import torch from math import sqrt . Tokenizer . Let&#39;s begin by looking at the tokenizer. It is essentially transforming your text into numerical values that your transformer will be able to &quot;understand&quot;. . Exercise: . Load the tokenizer for bert-base-uncased from HuggingFace | Tokenize the following text and return it as a PyTorch Tensor: &quot;time flies like an arrow&quot; | Print the input ids of the tokenized text | . Your Code: . model_ckpt = tokenizer = text = &quot;time flies like an arrow&quot; inputs = . Solution: . model_ckpt = &quot;bert-base-uncased&quot; tokenizer = AutoTokenizer.from_pretrained(model_ckpt) text = &quot;time flies like an arrow&quot; inputs = tokenizer(text, return_tensors=&quot;pt&quot;, add_special_tokens=True) inputs.input_ids . . Model . Now, let&#39;s look at the architecture of the bert-base-uncased model. It is important that the model checkpoint is the same as the one you used to instantiate your tokenizer. . Exercise: . Load the pretrained model from HuggingFace | Take a look at the architecture and answer the following questions: How many BertLayers does the model have? | What is the dropout probability in the BertEmbeddings? | What is the hidden dimension of the position_embeddingsand token_type_embeddings? | . | . Your Code: . model = . Solution: . &quot;&quot;&quot; Answers: - How many BertLayers does the model have? - 12 - What is the dropout probability in the BertEmbeddings? - p=0.1 -&gt; 10% - What is the hidden dimension of the position_embeddings and token_type_embeddings? - 768 &quot;&quot;&quot; model = AutoModel.from_pretrained(model_ckpt) model . . Config . Another way to access the desired information about the model (and more) is by looking at the model configuration file provided by HuggingFace. . Exercise: . Load the configuration file and take a look at the config object | What activation functionis being used on the hidden layers? | Find the id of the pad_token | . Your Code: . config = . Solution: . config = AutoConfig.from_pretrained(model_ckpt) config . . Self Attention . Let&#39; unravel the driving force behind transformers, ow that we have looked at the tokenizer, model and the corresponding config file. Let&#39;s look at self-attention. . Here you can see the basic attention calcualtion. Think of $Q$, $K$ and $V$ as matrices/tensors. In the case of langage models, these will have the corresponding shape $( text{batch size}, text{length of input sequence}, text{embedding dimension})$ . $Q$ = query | $K$ = key | $V$ = value | $d_k$ = hidden dimension of key | . We call the calculation cross-attention if $Q$, $K$ and $V$ are different matrices. Conversely, we call it self-attention if $Q$, $K$ and $V$ are the same matrix e.g. $Q$ = $K$ = $V$ = $X$ . $ text{Cross-Attention}(Q,K,V) = {softmax}( frac{QK^T}{ sqrt d_k})V$ . $ text{Self-Attention}(X_1,X_2,X_3) = {softmax}( frac{XX^T}{ sqrt d_x})X$ . Exercise: . Complete the implementation of the scaled_dot_product_attention function | . Your Code: . def scaled_dot_product_attention(): &quot;&quot;&quot;Example function with types documented in the docstring. Args: query (torch.tensor): Query tensor. key (torch.tensor): Key tensor. value (torch.tensor): Value tensor. Returns: attention (torch.tensor): Scaled dot product attention. &quot;&quot;&quot; pass . Solution: . def scaled_dot_product_attention(query: torch.tensor, key: torch.tensor, value: torch.tensor) -&gt; torch.tensor: &quot;&quot;&quot;This function calculates the scaled dot product attention. It takes a query, key and value as input and returns a PyTorch tensor containing the attention values Args: query (torch.tensor): Query tensor. key (torch.tensor): Key tensor. value (torch.tensor): Value tensor. Returns: attention (torch.tensor): Scaled dot product attention. &quot;&quot;&quot; dim_k = query.size(-1) scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k) weights = F.softmax(scores, dim=-1) attention = torch.bmm(weights, value) return attention . . Attention Head . In the image bellow you can see how the scaled_dot_product_attention function is being used in Multi-Head Attention. . A single Attention Head implements the parts until scaled dot product attention. The notion of multiple heads comes into play, once we add h &gt; 1 heads, concatenatethem and apply a Linear function. . . Exercise: . Implement the AttentionHead class | . Your Code: . class AttentionHead(nn.Module): &quot;&quot;&quot; Implements a single attention head. Multiple heads can be concatenated to obtain Multi-Head Attention. &quot;&quot;&quot; def __init__(self): pass def forward(self): pass . Solution: . class AttentionHead(nn.Module): &quot;&quot;&quot; Implements a single attention head. Multiple heads can be concatenated to obtain Multi-Head Attention. &quot;&quot;&quot; def __init__(self, embed_dim: int, head_dim: int): super().__init__() self.q = nn.Linear(embed_dim, head_dim) self.k = nn.Linear(embed_dim, head_dim) self.v = nn.Linear(embed_dim, head_dim) def forward(self, hidden_state: torch.tensor) -&gt; torch.tensor: attn_outputs = scaled_dot_product_attention( self.q(hidden_state), self.k(hidden_state), self.v(hidden_state)) return attn_outputs . . Multi-Head Attention . Now we can use our single Attention-Head to implement the Multi-Head Attention. . . Exercise: . Implement the MultiHeadAttention class | . Your Code: . class MultiHeadAttention(nn.Module): &quot;&quot;&quot;Creates Multi-Head Attention Layer by determining embedding dimension, number of heads and head dimension from the config file. &quot;&quot;&quot; def __init__(self): pass def forward(self): pass . Solution: . class MultiHeadAttention(nn.Module): &quot;&quot;&quot;Creates Multi-Head Attention Layer by determining embedding dimension, number of heads and head dimension from the config file. &quot;&quot;&quot; def __init__(self, config: dict): super().__init__() embed_dim = config.hidden_size num_heads = config.num_attention_heads head_dim = embed_dim // num_heads self.heads = nn.ModuleList( [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)] ) self.output_linear = nn.Linear(embed_dim, embed_dim) def forward(self, hidden_state: torch.tensor) -&gt; torch.tensor: x = torch.cat([h(hidden_state) for h in self.heads], dim=-1) x = self.output_linear(x) return x . . Feed Forward . Great, looking at the following diagram, we can see that we successfully implemented the orange Multi-Head Attention box. . But that is only half of the picture. Let&#39;s implement the other parts of a Transformer Encoder. . . Exercise: . Implement the Feed Forward class | It should contain: 2 linear layers with hidden_sizeand intermediate_size as found in the config file | A GELU activation function | A Dropoutlayer with the propper dropout probability | . | . Your Code: . class FeedForward(nn.Module): &quot;&quot;&quot;Implements feed forward layer with GeLU and Dropout. Contains 2 Linear layers. &quot;&quot;&quot; def __init__(self): pass def forward(self): pass . Solution: . class FeedForward(nn.Module): &quot;&quot;&quot;Implements feed forward layer with GeLU and Dropout. Contains 2 Linear layers. &quot;&quot;&quot; def __init__(self, config: dict): super().__init__() self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size) self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size) self.gelu = nn.GELU() self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, x: torch.tensor) -&gt; torch.tensor: x = self.linear_1(x) x = self.gelu(x) x = self.linear_2(x) x = self.dropout(x) return x . . Encoder Layer . With the Multi-Head Attention and Feed Forward classes in place, we can put together an entire encoder layerwhich corresponds to the gray box. . . Exercise: . Implement the TransformerEncoderLayer class | It should contain: Multi-Head Attention (orange box) | 2 Layer Norms (what we call Normin the yellow box) | Residual connections (what we call Add in the yellow box) | Feed Forward layer (blue box) | . | . Your Code: . class TransformerEncoderLayer(nn.Module): &quot;&quot;&quot; Combines Multi-Head Attention with Layer Normalization, Feed Forward Layer and skip connections to obtain an Encoder Layer. &quot;&quot;&quot; def __init__(self): pass def forward(self): pass . Solution: . class TransformerEncoderLayer(nn.Module): &quot;&quot;&quot; Combines Multi-Head Attention with Layer Normalization, Feed Forward Layer and skip connections to obtain an Encoder Layer. &quot;&quot;&quot; def __init__(self, config): super().__init__() self.layer_norm_1 = nn.LayerNorm(config.hidden_size) self.layer_norm_2 = nn.LayerNorm(config.hidden_size) self.attention = MultiHeadAttention(config) self.feed_forward = FeedForward(config) def forward(self, x): # Apply attention with a skip connection hidden_state = self.attention(x) # Apply layer normalization and then copy input into query, key, value intermediate_output = self.layer_norm_1(hidden_state + x) # Apply feed-forward layer with a skip connection output = self.layer_norm_2(intermediate_output + self.feed_forward(intermediate_output)) return output . . Embeddings . We&#39;re nearly there. . To give our transformer a chance at understanding our input tokens, we want to embedd the input token sequence into vectors. This way we can capture more information. . Since data is fed to a transformer at once - unlike sequentially as in an RNN - it doesn&#39;t know anything about the position of words in a given sentence. That&#39;s where the positional encoding comes in. It gives the transformer the ability to learn the positions. . . Exercise: . Implement the Embeddings class | It should contain: Token Embeddings (pink box) | Positional Embeddings (Circle with sin wave) | Dropout | Layer Norm | . | . Your Code: . class Embeddings(nn.Module): &quot;&quot;&quot;Implements token embeddings and positional embbeddings. &quot;&quot;&quot; def __init__(self): pass def forward(self): pass . Solution: . class Embeddings(nn.Module): &quot;&quot;&quot;Implements token embeddings and positional embbeddings. &quot;&quot;&quot; def __init__(self, config: dict): super().__init__() self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size) self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size) self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12) self.dropout = nn.Dropout() def forward(self, input_ids: torch.tensor) -&gt; torch.tensor: # Create position IDs for input sequence seq_length = input_ids.size(1) position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0) # Create token and position embeddings token_embeddings = self.token_embeddings(input_ids) position_embeddings = self.position_embeddings(position_ids) # Combine token and position embeddings embeddings = token_embeddings + position_embeddings embeddings = self.layer_norm(embeddings) embeddings = self.dropout(embeddings) return embeddings . . Transformer Encoder . Now for the full transformer Encoder the last missing piece is stacking up the Encoder Layers. In the diagram this is specified by N x . . Exercise: . Implement the TransformerEncoder class by combining the Embeddings and the Transformer Encoder Layers and stacking them up. | . Your Code: . class TransformerEncoder(nn.Module): &quot;&quot;&quot;Combines the Embedding layer and multiple encoder layers to obtain full transformer encoder. &quot;&quot;&quot; def __init__(self): pass def forward(self): pass . Solution: . class TransformerEncoder(nn.Module): &quot;&quot;&quot;Combines the Embedding layer and multiple encoder layers to obtain full transformer encoder. &quot;&quot;&quot; def __init__(self, config): super().__init__() self.embeddings = Embeddings(config) self.layers = nn.ModuleList( [TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)]) def forward(self, x): x = self.embeddings(x) for layer in self.layers: x = layer(x) return x . . Classification Head . Now that we have the transformer body, it&#39;d be great to add a head and make the transformer do something useful. . A common task in classification . Exercise: . Implement the TransformerForSequenceClassification head by adding a Linear layer on top of the body. | Set the output size to the desired number of class labels | . Your Code: . class TransformerForSequenceClassification(nn.Module): def __init__(self): pass def forward(self): pass . Solution: . class TransformerForSequenceClassification(nn.Module): def __init__(self, config): super().__init__() self.encoder = TransformerEncoder(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, config.num_labels) def forward(self, x): x = self.encoder(x)[:, 0, :] # select hidden state of [CLS] token x = self.dropout(x) x = self.classifier(x) return x . . Masked Attention . Last but not least, if we wanted to implement the Decoder of a transformer, we&#39;d need a way to mask our inputs. This is crucial, since the transformer could just hackthe decoding task by looking at the next word and using it as it&#39;s prediciton. . We can circumvent this issue by adding a mask to our scaled_dot_product_attention function. . Exercise: . Create a maks as a lower triangular matrix with it&#39;s upper values set to -inf | Change the scaled_dot_product_attention such that it can incorporate a mask. | . Your Code: . mask = . def scaled_dot_product_attention(query: torch.tensor, key: torch.tensor, value: torch.tensor, mask=None) -&gt; torch.tensor: &quot;&quot;&quot;This function calculates the scaled dot product attention. It takes a query, key and value as input and returns a PyTorch tensor containing the attention values. It includes a mask that can be used in a transformer decoder. Args: query (torch.tensor): Query tensor. key (torch.tensor): Key tensor. value (torch.tensor): Value tensor. mask (torch.tensor): Attention Mask. Returns: attention (torch.tensor): Scaled dot product attention. &quot;&quot;&quot; pass . Solution: . # Define mask a = torch.rand(1,5,768) mask = torch.tril(a) a.masked_fill(mask == 0, float(&#39;-inf&#39;)) . . def scaled_dot_product_attention(query: torch.tensor, key: torch.tensor, value: torch.tensor, mask=None) -&gt; torch.tensor: &quot;&quot;&quot;This function calculates the scaled dot product attention. It takes a query, key and value as input and returns a PyTorch tensor containing the attention values. It includes a mask that can be used in a transformer decoder. Args: query (torch.tensor): Query tensor. key (torch.tensor): Key tensor. value (torch.tensor): Value tensor. mask (torch.tensor): Attention Mask. Returns: attention (torch.tensor): Scaled dot product attention. &quot;&quot;&quot; dim_k = query.size(-1) scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k) if mask is not None: scores = scores.masked_fill(mask == 0, float(&quot;-inf&quot;)) weights = F.softmax(scores, dim=-1) return weights.bmm(value) . .",
            "url": "https://kaykozaronek.github.io/blog/attention%20is%20all%20you%20need/transformer/pytorch/nlp/coding%20exercise/huggingface/2022/07/15/Transformers-from-Scratch.html",
            "relUrl": "/attention%20is%20all%20you%20need/transformer/pytorch/nlp/coding%20exercise/huggingface/2022/07/15/Transformers-from-Scratch.html",
            "date": " • Jul 15, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Exercise - Encoder Decoder Transformer",
            "content": "&quot;This exercise guides you to a full implementation of a transformer with Pytorch and HuggingFace&quot; . Imports . from transformers import AutoTokenizer from transformers import AutoModel from transformers import AutoConfig from torch import nn import torch.nn.functional as F import torch from math import sqrt . Tokenizer . Exercise: . Load the tokenizer for bert-base-uncased | Print the input ids of the text: &quot;time flies like an arrow&quot; | . Your Code . Solution . #collapse-output model_ckpt = &quot;bert-base-uncased&quot; tokenizer = AutoTokenizer.from_pretrained(model_ckpt) text = &quot;time flies like an arrow&quot; inputs = tokenizer(text, return_tensors=&quot;pt&quot;, add_special_tokens=False) inputs.input_ids . . tensor([[ 2051, 10029, 2066, 2019, 8612]]) . Model . Exercise: . Load the pretrained model | Take a look at the architecture | . Your Code . Solution . #collapse-output model = AutoModel.from_pretrained(model_ckpt) model . . Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: [&#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.decoder.weight&#39;] - This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). . BertModel( (embeddings): BertEmbeddings( (word_embeddings): Embedding(30522, 768, padding_idx=0) (position_embeddings): Embedding(512, 768) (token_type_embeddings): Embedding(2, 768) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) ) . Config . Exercise: . Load the configuration file | Take a look at the config object | Find the hidden size of the model | . Your Code . Solution . config = AutoConfig.from_pretrained(model_ckpt) config . . BertConfig { &#34;_name_or_path&#34;: &#34;bert-base-uncased&#34;, &#34;architectures&#34;: [ &#34;BertForMaskedLM&#34; ], &#34;attention_probs_dropout_prob&#34;: 0.1, &#34;classifier_dropout&#34;: null, &#34;gradient_checkpointing&#34;: false, &#34;hidden_act&#34;: &#34;gelu&#34;, &#34;hidden_dropout_prob&#34;: 0.1, &#34;hidden_size&#34;: 768, &#34;initializer_range&#34;: 0.02, &#34;intermediate_size&#34;: 3072, &#34;layer_norm_eps&#34;: 1e-12, &#34;max_position_embeddings&#34;: 512, &#34;model_type&#34;: &#34;bert&#34;, &#34;num_attention_heads&#34;: 12, &#34;num_hidden_layers&#34;: 12, &#34;pad_token_id&#34;: 0, &#34;position_embedding_type&#34;: &#34;absolute&#34;, &#34;transformers_version&#34;: &#34;4.20.1&#34;, &#34;type_vocab_size&#34;: 2, &#34;use_cache&#34;: true, &#34;vocab_size&#34;: 30522 } . Self Attention . Exercise: . Write the scaled_dot_product_attentionfunction | . . Your Code . def scaled_dot_product_attention(): &quot;&quot;&quot;Example function with types documented in the docstring. Args: query (torch.tensor): Query tensor. key (torch.tensor): Key tensor. value (torch.tensor): Value tensor. Returns: attention (torch.tensor): Scaled dot product attention. &quot;&quot;&quot; pass . Solution . def scaled_dot_product_attention(query, key, value): &quot;&quot;&quot;Example function with types documented in the docstring. Args: query (torch.tensor): Query tensor. key (torch.tensor): Key tensor. value (torch.tensor): Value tensor. Returns: attention (torch.tensor): Scaled dot product attention. &quot;&quot;&quot; dim_k = query.size(-1) scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k) weights = F.softmax(scores, dim=-1) return torch.bmm(weights, value) . . Attention Head . Your Code . class AttentionHead(nn.Module): &quot;&quot;&quot; Implements a single attention head. Multiple heads can be used in Multi-Head Attention. &quot;&quot;&quot; def __init__(self): pass def forward(self): pass . . Solution . class AttentionHead(nn.Module): &quot;&quot;&quot; Implements a single attention head. Multiple heads can be used in Multi-Head Attention. &quot;&quot;&quot; def __init__(self, embed_dim, head_dim): super().__init__() self.q = nn.Linear(embed_dim, head_dim) self.k = nn.Linear(embed_dim, head_dim) self.v = nn.Linear(embed_dim, head_dim) def forward(self, hidden_state): attn_outputs = scaled_dot_product_attention( self.q(hidden_state), self.k(hidden_state), self.v(hidden_state)) return attn_outputs . . Multi-Head Attention . Your Code . class MultiHeadAttention(nn.Module): &quot;&quot;&quot;Creates Multi-Head Attention Layer by determining embedding dimension, number of heads and head dimension from the config file. &quot;&quot;&quot; def __init__(self): pass def forward(self): pass . . Solution . class MultiHeadAttention(nn.Module): &quot;&quot;&quot;Creates Multi-Head Attention Layer by determining embedding dimension, number of heads and head dimension from the config file. &quot;&quot;&quot; def __init__(self, config): super().__init__() embed_dim = config.hidden_size num_heads = config.num_attention_heads head_dim = embed_dim // num_heads self.heads = nn.ModuleList( [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)] ) self.output_linear = nn.Linear(embed_dim, embed_dim) def forward(self, hidden_state): x = torch.cat([h(hidden_state) for h in self.heads], dim=-1) x = self.output_linear(x) return x . . Feed Forward . Your Code . class FeedForward(nn.Module): &quot;&quot;&quot;Implements feed forward layer with GeLU and Dropout. Contains 2 Linear layers. &quot;&quot;&quot; def __init__(self): pass def forward(self): pass . Solution . class FeedForward(nn.Module): &quot;&quot;&quot;Implements feed forward layer with GeLU and Dropout. Contains 2 Linear layers. &quot;&quot;&quot; def __init__(self, config): super().__init__() self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size) self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size) self.gelu = nn.GELU() self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, x): x = self.linear_1(x) x = self.gelu(x) x = self.linear_2(x) x = self.dropout(x) return x . . Encoder Layer . Your Code . class TransformerEncoderLayer(nn.Module): &quot;&quot;&quot; Combines Multi-Head Attention with Layer Normalization, Feed Forward Layer and skip connections to obtain an Encoder Layer. &quot;&quot;&quot; def __init__(self): pass def forward(self): pass . Solution . class TransformerEncoderLayer(nn.Module): &quot;&quot;&quot; Combines Multi-Head Attention with Layer Normalization, Feed Forward Layer and skip connections to obtain an Encoder Layer. &quot;&quot;&quot; def __init__(self, config): super().__init__() self.layer_norm_1 = nn.LayerNorm(config.hidden_size) self.layer_norm_2 = nn.LayerNorm(config.hidden_size) self.attention = MultiHeadAttention(config) self.feed_forward = FeedForward(config) def forward(self, x): # Apply layer normalization and then copy input into query, key, value hidden_state = self.layer_norm_1(x) # Apply attention with a skip connection x = x + self.attention(hidden_state) # Apply feed-forward layer with a skip connection x = x + self.feed_forward(self.layer_norm_2(x)) return x . . Embeddings . Your Code . class Embeddings(nn.Module): &quot;&quot;&quot;Implements token embeddings and positional embbeddings. &quot;&quot;&quot; def __init__(self): pass def forward(self): pass . . Solution . class Embeddings(nn.Module): &quot;&quot;&quot;Implements token embeddings and positional embbeddings. &quot;&quot;&quot; def __init__(self, config): super().__init__() self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size) self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size) self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12) self.dropout = nn.Dropout() def forward(self, input_ids): # Create position IDs for input sequence seq_length = input_ids.size(1) position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0) # Create token and position embeddings token_embeddings = self.token_embeddings(input_ids) position_embeddings = self.position_embeddings(position_ids) # Combine token and position embeddings embeddings = token_embeddings + position_embeddings embeddings = self.layer_norm(embeddings) embeddings = self.dropout(embeddings) return embeddings . . Transformer Encoder . Your Code . class TransformerEncoder(nn.Module): &quot;&quot;&quot;Combines the Embedding layer and multiple encoder layers to obtain full transformer encoder. &quot;&quot;&quot; def __init__(self): pass def forward(self): pass . Solution . class TransformerEncoder(nn.Module): &quot;&quot;&quot;Combines the Embedding layer and multiple encoder layers to obtain full transformer encoder. &quot;&quot;&quot; def __init__(self, config): super().__init__() self.embeddings = Embeddings(config) self.layers = nn.ModuleList( [TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)]) def forward(self, x): x = self.embeddings(x) for layer in self.layers: x = layer(x) return x . . Classification Head . Your Code . class TransformerForSequenceClassification(nn.Module): def __init__(self): pass def forward(self): pass . . Solution . class TransformerForSequenceClassification(nn.Module): def __init__(self, config): super().__init__() self.encoder = TransformerEncoder(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, config.num_labels) def forward(self, x): x = self.encoder(x)[:, 0, :] # select hidden state of [CLS] token x = self.dropout(x) x = self.classifier(x) return x . . Masked Attention . Exercise: . Change the scaled_dot_product_attention such that it can incorporate a mask. | . Your Code . def scaled_dot_product_attention(): pass . . Solution . def scaled_dot_product_attention(query, key, value, mask=None): dim_k = query.size(-1) scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k) if mask is not None: scores = scores.masked_fill(mask == 0, float(&quot;-inf&quot;)) weights = F.softmax(scores, dim=-1) return weights.bmm(value) . . Putting it all together: Encoder-Decoder Transformer . Your Code . Solution . . .",
            "url": "https://kaykozaronek.github.io/blog/attention%20is%20all%20you%20need/transformer/pytorch/nlp/coding%20exercise/2022/07/11/Exercise-Transformers.html",
            "relUrl": "/attention%20is%20all%20you%20need/transformer/pytorch/nlp/coding%20exercise/2022/07/11/Exercise-Transformers.html",
            "date": " • Jul 11, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Weekly Goals Update 03",
            "content": "Status: . Last week was great. I met a bunch of very intelligent, ambitious and intelectually curious people. The CHERI summer fellowship seems promising. I set up an accountability partners group, communicated my mentorship expectations and made good friends. . Goal Points Progress Total Points . 0 CHERI Introductory Week | 70 | 1.000000 | 70.000000 | . 1 Catch up with Anki | 5 | 1.000000 | 5.000000 | . 2 Read AGI Safety Fundamentals from first principles | 15 | 0.000000 | 0.000000 | . 3 Write down expectations for mentorship | 10 | 1.000000 | 10.000000 | . 4 Total | | | 85.000000 | . Goals: . This week my main goal is to reimplement the &quot;Training Language Models with Language Feedback&quot; paper. To do so, I will have to acquaint myself with the OpenAI API. . As a secondary goal, I want to work through the NYU NLP lecture and start watching the Deep RL Lecture Series (UC Berkeley). . Finally, it&#39;d be fun to check out the local EA chapter in Budapest. . At the end of the week I hope to converge on a stable weekly schedule I can follow throughout the summer fellowship. . Goal Points . 0 Work through OpenAI API guides | 30 | . 1 Reimplement &quot;training language models with lan... | 25 | . 2 Publish Transformer Exercise | 15 | . 3 NYU Deep Learning Course Week 12 | 20 | . 4 Attend EA meeting in Budapest | 5 | . 5 Create standard weekly schedule | 5 | . 6 Bonus: Check out Deep RL course | 5 | . 7 Total | | . Explore in upcoming week(s): . Mastering Pytorch | .",
            "url": "https://kaykozaronek.github.io/blog/reading/books/academic%20papers/blogs/mental%20diet/2022/07/10/Update-03-Weekly-Goals.html",
            "relUrl": "/reading/books/academic%20papers/blogs/mental%20diet/2022/07/10/Update-03-Weekly-Goals.html",
            "date": " • Jul 10, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Weekly Goals Update 02",
            "content": "Status: . Here&#39;s the progress of last weeks goals. . Goal Points Progress Total Points . 0 Reimplement &quot;Training Language Models with Language Feedback&quot; | 25 | 0.200000 | 5.000000 | . 1 Finish Software Engineering practices for Data Scientists | 15 | 1.000000 | 15.000000 | . 2 Create blog | 25 | 1.000000 | 25.000000 | . 3 Apply to Future Forum | 10 | 1.000000 | 10.000000 | . 4 Apply to Future Academy | 5 | 1.000000 | 5.000000 | . 5 Code Proximal Policy Optimization (PPO) | 5 | 0.500000 | 2.500000 | . 6 Code Generalized Advantage Estimation (GAE) | 5 | 1.000000 | 5.000000 | . 7 Creat Github repo for CHERI project | 5 | 0.000000 | 0.000000 | . 8 Do CHERI project planning | 5 | 1.000000 | 5.000000 | . 9 Total | | | 72.500000 | . Goals: . This week is non representative. It&#39;s main focus is not on advancing my project, but rather getting it started and setting up the right procedures. A key focus of this week is to correctly establish my goals, the mentorship relationship and the &quot;&gt; 90%-final&quot; project plan. Besides that, there are a lot of prescheduled events by the CHERI organizers in which I will participate. The focus will be in building stong ties with my peers and ask them the Hamming question: “What are the important problems in your field, and why are(n&#39;t) you working on them? . Goal Points . 0 CHERI Introductory Week | 70 | . 1 Catch up with Anki | 5 | . 2 Read AGI Safety Fundamentals from first princi... | 15 | . 3 Write down expectations for mentorship | 10 | . 4 Total | | . Explore in upcoming week(s): . Explore OpenAI API | Deep RL Lecture Series (UC Berkeley) | NYU Pytorch Deep Learning | Mastering Pytorch | .",
            "url": "https://kaykozaronek.github.io/blog/reading/books/academic%20papers/blogs/mental%20diet/2022/07/03/Update-02-Weekly-Goals.html",
            "relUrl": "/reading/books/academic%20papers/blogs/mental%20diet/2022/07/03/Update-02-Weekly-Goals.html",
            "date": " • Jul 3, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Reading List",
            "content": ". Tip: Feel free to suggest readings in the comment section. . Title Category Topic Progress . 0 Natural Language Processing with Transformers | Book | NLP | 100 | . 1 Mastering PyTorch | Book | Deep Learning | 0 | . 2 Self-critiquing models for assisting human evaluators | Academic Paper | NLP | 30 | . 3 Training Language Models with Language Feedback | Academic Paper | NLP | 50 | . 4 Learning to summarize from human feedback | Academic Paper | NLP | 30 | . 5 AGI Safety From First Principles | Blog | AI Safety | 0 | . 6 Propositions Concerning Digital Minds and Society | Academic Paper | Consciousness, Digital Minds | 50 | .",
            "url": "https://kaykozaronek.github.io/blog/reading/books/academic%20papers/blogs/mental%20diet/2022/07/02/Reading-List.html",
            "relUrl": "/reading/books/academic%20papers/blogs/mental%20diet/2022/07/02/Reading-List.html",
            "date": " • Jul 2, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "Exploring the OpenAI critiques dataset",
            "content": "Introduction . In this notebook I am taking a look at the new dataset released by OpenAI on the 13th of June in the paper: &quot;Self-critiquing models for assisting human evaluators&quot;. . Imports . from datasets import load_dataset import pandas as pd import matplotlib.pyplot as plt import torch import torch.nn.functional as F . The Data . 1. Base Dataset . The Base Dataset it structured like so: . Train . id | split | time | labeler | is_topic_based_summarization | data passage text | title | . | questions list of multiple question-answer pairs (dictionaries) | . | . | . | Test . etc. | . | . base_url = &quot;https://openaipublic.blob.core.windows.net/critiques/dataset/base/&quot; base_dataset = load_dataset(&#39;json&#39;, data_files={&#39;train&#39;: base_url + &#39;train.jsonl.gz&#39;, &#39;test&#39;: base_url + &#39;test.jsonl.gz&#39;}) base_dataset . Using custom data configuration default-b6bdeb08a451c657 Reusing dataset json (/root/.cache/huggingface/datasets/json/default-b6bdeb08a451c657/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5) . DatasetDict({ train: Dataset({ features: [&#39;id&#39;, &#39;split&#39;, &#39;time&#39;, &#39;labeler&#39;, &#39;is_topic_based_summarization&#39;, &#39;data&#39;], num_rows: 11232 }) test: Dataset({ features: [&#39;id&#39;, &#39;split&#39;, &#39;time&#39;, &#39;labeler&#39;, &#39;is_topic_based_summarization&#39;, &#39;data&#39;], num_rows: 3857 }) }) . . Figuring out the structure of the dataset . df = pd.DataFrame(base_dataset[&#39;train&#39;]) df.head() . id split time labeler is_topic_based_summarization data . 0 Hvwa1M6h1l81jmLcI2WdliTDayc6ik | train | 1.654295e+09 | 9d66ba714984b4ac37359c8a26b065d2d5e1d508b349a2... | True | {&#39;passage&#39;: {&#39;text&#39;: &#39;[Between now and the end... | . 1 tCYeeDnFlAqrHk9HHYV5BnC4JD5eYQ | train | 1.654295e+09 | d95e9d66406f3756657b3e159c883527a54ebe2d11fcb6... | False | {&#39;passage&#39;: {&#39;text&#39;: &#39; The hikers had left a m... | . 2 YfuuobgOYOdMtT7nSuaLFaJ7yRBjX6 | train | 1.654295e+09 | 8774b0664d5c0ab1502c35813c97e6ae44b477c0ac0a7c... | False | {&#39;passage&#39;: {&#39;text&#39;: &#39;CHARLESTON EXECUTIVE AIR... | . 3 b5KtNkpspmPl9Is6uAaDTBAGKFsUZg | train | 1.654295e+09 | c386a07c8ceed1b1dfcd2126015c772310f9ccf5c34a82... | False | {&#39;passage&#39;: {&#39;text&#39;: &#39;I reach into my pocket t... | . 4 08dDPy00rGm3wNIZPusqxSL8A8ADxu | train | 1.654295e+09 | 9217c5bbd255314f3a9222a5c253cc60571b546da6c393... | False | {&#39;passage&#39;: {&#39;text&#39;: &#39;Whatever had wiped out a... | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df[&#39;data&#39;][0].keys() . dict_keys([&#39;passage&#39;, &#39;questions&#39;]) . df[&#39;data&#39;][0][&#39;passage&#39;].keys() . dict_keys([&#39;text&#39;, &#39;title&#39;]) . df[&#39;data&#39;][0][&#39;passage&#39;][&#39;text&#39;] . &#34;[Between now and the end of the year, Justin Trudeau&#39;s capacity to forge a consensus in Parliament, on the federal-provincial front and, possibly, within his own caucus will be tested as the rubber meets the road on some key campaign commitments, writes Chantal Hébert.] n n[Chantal Hébert] n nChantal HébertNational Affairs Columnist n n__Sat., Nov. 19, 20163 min. read n nArticle was updated Nov. 18, 2016 n nIf you have been enjoying Canada’s comparatively cool political climate since Justin Trudeau became prime minister, make the most of what may be the last days of the season. n nBy all indications, the political temperature is about to rise as deadlines looms on three potentially troublesome fronts for the Liberal government. n nBetween now and the end of the year, the prime minister’s capacity to forge a consensus in Parliament, on the federal-provincial front and, possibly, within his own caucus will be tested as the rubber meets the road on some key campaign commitments. n nOn or before Dec. 1, the special committee that has been exploring a reform of Canada’s voting system will report its findings to the government. If the Liberals have a principled position on this issue, they have been doing a great job of keeping it under wraps. n nThe committee report should signal the beginning of the end of the Liberal game of hide-and-seek. n nThe opposition parties hold the majority at the electoral reform table and, in any event, no government is bound to implement the prescription of a committee. If such an obligation existed, Canada’s new law on medically assisted suicide would be a lot less restrictive. But if Trudeau is presented with an opposition consensus as to the way forward on the voting system he will, at a minimum, have to come up with the kind of coherent response that has been sorely lacking to date. n nThis week, democratic reform minister Maryam Monsef reported, on the basis of her own consultations, that there was no consensus within the public as to a preferred voting system. The representations made to the committee on the other hand have tended to favour a more proportional system. Consensus, in this instance, is very much in the opportunistic eye of the beholder. But more on that later in this column. n nThe odds of a majority committee report increased this week when the NDP signalled that it could support the Conservative call for any new voting system to be put to a national referendum. If there is solid majority within the electorate to be found for anything pertaining to electoral reform, it revolves around the notion that a change should be approved through a national plebiscite. n nOne way or another, it does seem that at least one part of Trudeau’s promise will not be fulfilled. In the still unlikely scenario that the Liberals sign off on a national plebiscite, the debate would shift to the rewriting of the federal referendum law and then to the actual holding of a national vote. Getting all that done within the time frame Elections Canada says it needs to put a different voting system in place for 2019 would be extremely difficult. And that is, of course, assuming a reform proposal wins the day. n nOn Dec. 9, Trudeau is tentatively scheduled to meet with the premiers to put the finishing touches on the country’s climate change strategy. The first ministers have not gathered since the prime minister signalled his intention to set a floor price on carbon. In the interval, Donald Trump’s victory and the expectation that his administration will not follow up on the Paris climate accord have added grist to the mill of opponents of a Canadian carbon tax. n nTrudeau does not lack for provincial allies on carbon pricing but the same is not true of his plan to cut the annual increase of the health transfer to 3 per cent. The prime minister wants to avoid a linkage between the two files. Absent some conciliatory federal move on health-care funding, that linkage may be hard to avoid next month and not just on carbon pricing. n nDec. 19 is the deadline for the federal cabinet to decide the fate of Kinder Morgan’s plan to increase the capacity of the TransMountain pipeline. It links Alberta to the coast off Vancouver. In the wake of the American election, Energy Minister Jim Carr has argued that Trump’s victory and the prospect of a revival of the Keystone XL project did not diminish the need for more pipeline capacity in Canada. n nTrudeau has long said he would not proceed with a pipeline absent a so-called social licence for the project. If his government applied to the quest of a pro-pipeline consensus in British Columbia the same loose criteria it is using to declare that there is no consensus in sight on electoral reform, the TransMountain pipeline would be dead on arrival. n nLoading... n nLoading...Loading...Loading...Loading...Loading... n&#34; . . df[&#39;data&#39;][0][&#39;questions&#39;] . [{&#39;answer&#39;: &#39;&#39;, &#39;question&#39;: &#39;What does the text say about National Affairs Columnist, Chantal Hébert?&#39;}, {&#39;answer&#39;: &#34;The prime minister&#39;s capacity to forge a consensus in Parliament, on the federal-provincial front and his own caucus will be tested as some key campaign commitments face their deadlines. There are three potentially troublesome fronts for Trudeau&#39;s Liberal government. He must come up with a coherent response to the voting system. &#34;, &#39;question&#39;: &#39;What does the text say about Canadian Prime Minister Justin Trudeau?&#39;}, {&#39;answer&#39;: &#34;On or before December 1, the special committee exploring reform of Canada&#39;s voting system will report its findings to the government. Democratic reform minister Maryam Monsef made her own consultations and found that there was no consensus within the public as to a preferred voting system. On December 9, Trudeau is tentatively scheduled to meet with the premiers to put the finishing touches on the country&#39;s climate change strategy. &#34;, &#39;question&#39;: &#39;Summarize everything that happened between December 1 to December 9.&#39;}, {&#39;answer&#39;: &#34;December 19 is the deadline for the federal cabinet to decide the fate of Kinder Morgan&#39;s plan to increase the capacity of the TransMountain pipeline. The pipeline links Alberta to the coast of Vancouver. However, Trudeau has long said that he would not proceed with a pipeline absent a social license for the project.&#34;, &#39;question&#39;: &#39;What does the text say about December 19th?&#39;}] . . Id, split and time . We expect each id to be unique | Since we&#39;re looking at just the training set we expect split to only have 1 unique value: &#39;train&#39; | I don&#39;t know yet how to interpret the time column, but I think it&#39;s not all too important: Could be how long did labeling this piece took | Or date and time at which labeler started labeling | . | . df[&#39;id&#39;].nunique() == len(df) . True . df[&#39;split&#39;].unique() . array([&#39;train&#39;], dtype=object) . df[&#39;time&#39;].nunique() . 11232 . df[&#39;time&#39;][0] . 1654294771.222219 . is_topic_based_summarization . This column tells us whether the summarization is topic based or not | What are the other options though? | . topic_based_df = df[&#39;is_topic_based_summarization&#39;].value_counts() topic_based_df.plot(kind=&#39;bar&#39;) plt.title(&#39;Count of (non) topic based summarizations&#39;) plt.ylabel(&#39;Frequency&#39;) plt.xlabel(&#39;Is topic based summarization&#39;); . print(f&#39;Topic based: {round(topic_based_df[False] / len(df) * 100, 2)} % of the data&#39;) print(f&#39;Not topic based: {round(topic_based_df[True] / len(df) * 100, 2)} % of the data&#39;) . Topic based: 80.23 % of the data Not topic based: 19.77 % of the data . labeler . How many unique labelers were there? | How many items did each labeler label | What was the mean amount each labeler labeled | . num_labelers = len(df[&#39;labeler&#39;].unique()) num_labelers . 35 . rename_labelers = [f&#39;labeler {i}&#39; for i in range(1, num_labelers +1)] len(rename_labelers) . 35 . df = df.replace({&#39;labeler&#39;: dict(zip(df[&#39;labeler&#39;].unique(), rename_labelers))}) df.head(2) . . id split time labeler is_topic_based_summarization data . 0 Hvwa1M6h1l81jmLcI2WdliTDayc6ik | train | 1.654295e+09 | labeler 1 | True | {&#39;passage&#39;: {&#39;text&#39;: &#39;[Between now and the end... | . 1 tCYeeDnFlAqrHk9HHYV5BnC4JD5eYQ | train | 1.654295e+09 | labeler 2 | False | {&#39;passage&#39;: {&#39;text&#39;: &#39; The hikers had left a m... | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; labeler_count_df = df.groupby(&#39;labeler&#39;).count()[&#39;data&#39;] labeler_count_df.sort_values().plot(kind=&#39;barh&#39;, figsize=(5,10)) mean_val = labeler_count_df.mean() plt.axvline(x=mean_val, color=&#39;r&#39;, linestyle=&#39;--&#39;, label=&#39;mean&#39;) plt.title(&#39;Labelers by contribution&#39;) plt.xlabel(&#39;Number of labeled data&#39;) plt.text(mean_val+50, 17,&#39;mean&#39;, c=&#39;red&#39;, rotation=45); . Let&#39;s finally look at our data . data.passage . How long are the texts on average? | How long are the titles on average? | . data = df[&#39;data&#39;] text_len = 0 title_len = 0 for datapoint in data: text_len += len(datapoint[&#39;passage&#39;][&#39;text&#39;]) title_len += len(datapoint[&#39;passage&#39;][&#39;title&#39;]) if datapoint[&#39;passage&#39;][&#39;title&#39;] != None else 0 print(f&#39;Mean text length: {int(text_len / len(data))} words&#39;) print(f&#39;Mean title length: {int(title_len / len(data))} words&#39;) . Mean text length: 4563 words Mean title length: 19 words . print(data[1][&#39;passage&#39;][&#39;text&#39;][:100]) . The hikers had left a map of the area which gave us a vast layout of the land. It wasn&#39;t good. Ther . data[1][&#39;passage&#39;][&#39;title&#39;] . &#39;The Walking Dead: Winter Chronicles nChapter Four: Five Sisters&#39; . data.questions . How do the answers and qustions look like? | How many question answer pairs are there per datapoint? | . df[&#39;data&#39;][0][&#39;questions&#39;] . [{&#39;answer&#39;: &#39;&#39;, &#39;question&#39;: &#39;What does the text say about National Affairs Columnist, Chantal Hébert?&#39;}, {&#39;answer&#39;: &#34;The prime minister&#39;s capacity to forge a consensus in Parliament, on the federal-provincial front and his own caucus will be tested as some key campaign commitments face their deadlines. There are three potentially troublesome fronts for Trudeau&#39;s Liberal government. He must come up with a coherent response to the voting system. &#34;, &#39;question&#39;: &#39;What does the text say about Canadian Prime Minister Justin Trudeau?&#39;}, {&#39;answer&#39;: &#34;On or before December 1, the special committee exploring reform of Canada&#39;s voting system will report its findings to the government. Democratic reform minister Maryam Monsef made her own consultations and found that there was no consensus within the public as to a preferred voting system. On December 9, Trudeau is tentatively scheduled to meet with the premiers to put the finishing touches on the country&#39;s climate change strategy. &#34;, &#39;question&#39;: &#39;Summarize everything that happened between December 1 to December 9.&#39;}, {&#39;answer&#39;: &#34;December 19 is the deadline for the federal cabinet to decide the fate of Kinder Morgan&#39;s plan to increase the capacity of the TransMountain pipeline. The pipeline links Alberta to the coast of Vancouver. However, Trudeau has long said that he would not proceed with a pipeline absent a social license for the project.&#34;, &#39;question&#39;: &#39;What does the text say about December 19th?&#39;}] . . Look at the first question-answer pair . The question being asked does not have an answer in the accompanying text, thus the answer column is empty | . len_question_answer_pairs = df[&#39;data&#39;].apply(lambda x: len(x[&#39;questions&#39;])) len_question_answer_pairs . 0 4 1 5 2 5 3 5 4 6 .. 11227 5 11228 4 11229 4 11230 5 11231 6 Name: data, Length: 11232, dtype: int64 . len_question_answer_pairs.value_counts().sort_values().loc[[i for i in range(1,10)]].plot(kind=&#39;bar&#39;) plt.title(&#39;Question-Answer pairs by frequency&#39;) plt.xlabel(&#39;Number of Question-Answer Pairs&#39;) plt.ylabel(&#39;Frequency&#39;); .",
            "url": "https://kaykozaronek.github.io/blog/gpt-3/language%20models/ai%20safety/ai%20alignment/dataset/openai/2022/06/29/Exploring-OpenAI's-Critique-Dataset.html",
            "relUrl": "/gpt-3/language%20models/ai%20safety/ai%20alignment/dataset/openai/2022/06/29/Exploring-OpenAI's-Critique-Dataset.html",
            "date": " • Jun 29, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Weekly Goals Update 01",
            "content": "Status: . Here&#39;s the progress of last weeks goals. . Goal Points Progress Total Points . 0 Finish reading Transformer Book | 30 | 1.000000 | 30.000000 | . 1 Read &quot;Self-critiquing models for assisting human evaluators&quot; | 20 | 1.000000 | 20.000000 | . 2 Read &quot;Learning to summarize from human feedback&quot; | 10 | 1.000000 | 10.000000 | . 3 Read &quot;Training Language Models with Language Feedback&quot; | 20 | 1.000000 | 20.000000 | . 4 Explore Critique Dataset in Notebook | 10 | 0.000000 | 0.000000 | . 5 Code REINFORCE | 5 | 1.000000 | 5.000000 | . 6 Code: A2C | 5 | 1.000000 | 5.000000 | . 7 Total | | | 90.000000 | . Goals: . This week is a little more scattered. I&#39;m helping out a friend who&#39;s created a course to test it and provide feedback. I also want to play around with different ideas of how I can make my CHERI project the most effective. I want it to be a great sample of my work, both in writing and coding, irrespective of the final outcome. That means that I should continuously create code and written artifacts that I will be able to present as a “portfolio”. Building a blog for this purpose seems like something worth considering. . Goal Points . 0 Reimplement &quot;Training Language Models with Lan... | 25 | . 1 Finish Software Engineering practices for Data... | 15 | . 2 Create blog | 25 | . 3 Apply to Future Forum | 10 | . 4 Apply to Future Academy | 5 | . 5 Code Proximal Policy Optimization (PPO) | 5 | . 6 Code Generalized Advantage Estimation (GAE) | 5 | . 7 Creat Github repo for CHERI project | 5 | . 8 Do CHERI project planning | 5 | . 9 Total | | . Explore in upcoming week(s): . Code Karpathy MinGPT | Code MLAB GPT | Deep RL Lecture Series (UC Berkeley) | NYU Pytorch Deep Learning | .",
            "url": "https://kaykozaronek.github.io/blog/reading/books/academic%20papers/blogs/mental%20diet/2022/06/26/Update-01-Weekly-Goals.html",
            "relUrl": "/reading/books/academic%20papers/blogs/mental%20diet/2022/06/26/Update-01-Weekly-Goals.html",
            "date": " • Jun 26, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://kaykozaronek.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website chronicles my exploration of learning in machines and humans. .",
          "url": "https://kaykozaronek.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kaykozaronek.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}